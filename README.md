# OmniAI::Mistral

[![CircleCI](https://circleci.com/gh/ksylvest/omniai-mistral.svg?style=svg)](https://circleci.com/gh/ksylvest/omniai-mistral)

An Mistral implementation of the [OmniAI](https://github.com/ksylvest/omniai) APIs.

## Installation

```sh
gem install omniai-mistral
```

## Usage

### Client

A client is setup as follows if `ENV['MISTRAL_API_KEY']` exists:

```ruby
client = OmniAI::Mistral::Client.new
```

A client may also be passed the following options:

- `api_key` (required - default is `ENV['MISTRAL_API_KEY']`)
- `host` (optional)

### Configuration

Global configuration is supported for the following options:

```ruby
OmniAI::Mistral.configure do |config|
  config.api_key = 'sk-...' # default: ENV['MISTRAL_API_KEY']
  config.host = '...' # default: 'https://api.mistral.ai'
end
```

### Chat

A chat completion is generated by passing in prompts using any a variety of formats:

```ruby
completion = client.chat('Tell me a joke!')
completion.choice.message.content # 'Why did the chicken cross the road? To get to the other side.'
```

```ruby
completion = client.chat({
  role: OmniAI::Chat::Role::USER,
  content: 'Is it wise to jump off a bridge?'
})
completion.choice.message.content # 'No.'
```

```ruby
completion = client.chat([
  {
    role: OmniAI::Chat::Role::SYSTEM,
    content: 'You are a helpful assistant.'
  },
  'What is the capital of Canada?',
])
completion.choice.message.content # 'The capital of Canada is Ottawa.'
```

#### Model

`model` takes an optional string (default is `mistral-medium-latest`):

```ruby
completion = client.chat('Provide code for fibonacci', model: OmniAI::Mistral::Chat::Model::CODESTRAL)
completion.choice.message.content # 'def fibonacci(n)...end'
```

[Mistral API Reference `model`](https://docs.mistral.ai/getting-started/models/)

#### Temperature

`temperature` takes an optional float between `0.0` and `1.0` (defaults is `0.7`):

```ruby
completion = client.chat('Pick a number between 1 and 5', temperature: 1.0)
completion.choice.message.content # '3'
```

[Mistral API Reference `temperature`](https://docs.mistral.ai/api/)

#### Stream

`stream` takes an optional a proc to stream responses in real-time chunks instead of waiting for a complete response:

```ruby
stream = proc do |chunk|
  print(chunk.choice.delta.content) # 'Better', 'three', 'hours', ...
end
client.chat('Be poetic.', stream:)
```

[Mistral API Reference `stream`](https://docs.mistral.ai/api/)

#### Format

`format` takes an optional symbol (`:json`) and that sets the `response_format` to `json_object`:

```ruby
completion = client.chat([
  { role: OmniAI::Chat::Role::SYSTEM, content: OmniAI::Chat::JSON_PROMPT },
  { role: OmniAI::Chat::Role::USER, content: 'What is the name of the drummer for the Beatles?' }
], format: :json)
JSON.parse(completion.choice.message.content) # { "name": "Ringo" }
```

[Mistral API Reference `response_format`](https://docs.mistral.ai/api/)

> When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message.
